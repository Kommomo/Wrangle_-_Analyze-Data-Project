{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b010f5af",
   "metadata": {},
   "source": [
    "# Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9c645",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This report documents the wrangling efforts performed on 3 datasets of twitter from the tweet archive of Twitter user @dog_rates, also known as WeRateDogs. WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These datasets are named twitter_archive_enhanced.csv, image-predictions.tsv and tweet-json.txt \n",
    "Each of these datasets which are tweet data have characteristics of big data making them have data quality and tidiness issues as will be seen by their dirtiness and messiness. In a bid to inspect these dataset and fix these issues i will put these datasets through the wrangling process in a 3-fold step summarized as - \n",
    "- Gather\n",
    "- Assess and \n",
    "- Clean\n",
    "\n",
    "The result of this process will be a cleaned dataset.\n",
    "\n",
    "Note that the practical wrangling process is carried out in the wrangle_act.ipynb file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac306060",
   "metadata": {},
   "source": [
    "First i will import all the python packages required to successfully perform wrangling. The libraries required include: numpy, pandas, matplotlib,seaborn, tweepy, requests, os, json, regular expressions(re)etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cec1a73",
   "metadata": {},
   "source": [
    "### Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4189cd",
   "metadata": {},
   "source": [
    "Here, i read and loaded the 3 datasets into my pandas dataframe. \n",
    "- The twitter_archive_enhanced csv file stores information on the WerateDogs twitter archive\n",
    "- The image-predictions tsv file stores information on the kind of dog present in each tweet according to a neural network. It is hosted on Udacity's servers and i downloaded it programmatically using the Requests library and the following URL: https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv\n",
    "- The additional data from the tweeter API is stored in a tweet-json txt file and i downloaded from Udacity's link as my application for a twitter developer account is still under review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c634978",
   "metadata": {},
   "source": [
    "### Data Assessing\n",
    "\n",
    "> On successfully gathering all necessary data, assessing of data is carried out both Visually using Microsoft Excel spreadsheet program and Programmatically using codes. The following Quality and tidy issues were detected-\n",
    "\n",
    "#### Quality\n",
    "`Twitter_archive` table\n",
    "- Retweet entries\n",
    "- Wrong data type for timestamp \n",
    "- Erroneous data type for (in_reply_to_status_id and in_reply_to_user_id columns). \n",
    "- Html tags are displayed as part of the source entries\n",
    "- Inconsistent rating_denominator entries- 10 sometimes and less(0) or more(170) other times.\n",
    "- Several extraneous columns with missing entries (in_reply_to_status_id,in_reply_to_user_id, text, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp and expanded_urls) \n",
    "\n",
    "`Image_Prediction` table\n",
    "- Duplicates entries in jpg_url column\n",
    "- Inconsistent prediction data entry in p1, p2 and p3-lowercase sometimes and uppercase other times\n",
    "\n",
    "#### Tidiness\n",
    "- Multiple column representations for dog_stage\n",
    "- Seperate column representations for rating_numerator and rating_denominator in `twitter_archive`\n",
    "- image predictions and retweet count and favourite count should be part of `twitter_archive` table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561672b",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "> After a thorough assessment and detection of content and structural issues in the datasets, the cleaning process is carried out to fix each of these identified issues. \n",
    "\n",
    "> Before commencing the cleaning process, the twitter_archive_enhanced.csv, image-predictions.tsv and tweet-json.txt datasets consisted of 2356,2075 and 2354 entries respectively. \n",
    "\n",
    "> By the end of the cleaning process, they are merged into one dataset file called twitter_werate and then stored in a file called twitter_archive_master.csv. This dataset consists of 1954 entries with all the detected quality and tidiness issues resolved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
